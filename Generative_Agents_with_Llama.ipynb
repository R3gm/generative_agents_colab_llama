{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R3gm/generative_agents_llama/blob/main/Generative_Agents_with_Llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng4xDrNglwDM"
      },
      "source": [
        "# Generative Agents with Llama 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://joonsungpark.s3.amazonaws.com:443/static/assets/characters/profile/Isabella_Rodriguez.png\" alt=\"Generative Isabella\">\n",
        " <img src=\"https://joonsungpark.s3.amazonaws.com:443/static/assets/characters/profile/Sam_Moore.png\" alt=\"Generative Sam\">\n",
        " <img src=\"https://joonsungpark.s3.amazonaws.com:443/static/assets/characters/profile/Eddy_Lin.png\" alt=\"Generative Eddy\">\n",
        " <img src=\"https://joonsungpark.s3.amazonaws.com:443/static/assets/characters/profile/Wolfgang_Schulz.png\" alt=\"Generative Wolfgang\">"
      ],
      "metadata": {
        "id": "FPVlsGmrhRr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to use open-source models in agent simulation."
      ],
      "metadata": {
        "id": "FfF0DnaOg-wV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20XzuihslyUt"
      },
      "source": [
        "## Clone the repository"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/R3gm/generative_agents_llama.git"
      ],
      "metadata": {
        "id": "TXPCppW6LIRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install dependencies"
      ],
      "metadata": {
        "id": "xM7IHaUeV7cO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd generative_agents_llama\n",
        "!python -m pip install -r requirements.txt\n",
        "\n",
        "import torch\n",
        "import os\n",
        "try:\n",
        "  from llama_cpp import Llama\n",
        "except:\n",
        "  if torch.cuda.is_available():\n",
        "      print(\"CUDA is available on this system.\")\n",
        "      os.system('CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose')\n",
        "  else:\n",
        "      print(\"CUDA is not available on this system.\")\n",
        "      os.system('pip install llama-cpp-python')"
      ],
      "metadata": {
        "id": "4yaIxcmzq6fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure the model\n",
        "\n",
        "You can change the following parameters to choose the model you require.\n",
        "\n",
        "```\n",
        "repo_model = \"TheBloke/OpenOrca-Platypus2-13B-GGML\"\n",
        "filename_model = \"openorca-platypus2-13b.ggmlv3.q4_1.bin\"\n",
        "temperature_model = 0.5\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "acNb3r_9WGEb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXj-nvsOmI1j"
      },
      "outputs": [],
      "source": [
        "%%writefile reverie/backend_server/utils.py\n",
        "# Select a model\n",
        "repo_model = \"TheBloke/OpenOrca-Platypus2-13B-GGML\"\n",
        "filename_model = \"openorca-platypus2-13b.ggmlv3.q4_1.bin\"\n",
        "temperature_model = 0.5\n",
        "\n",
        "# Put your name\n",
        "key_owner = \"Generic_User\"\n",
        "\n",
        "maze_assets_loc = \"../../environment/frontend_server/static_dirs/assets\"\n",
        "env_matrix = f\"{maze_assets_loc}/the_ville/matrix\"\n",
        "env_visuals = f\"{maze_assets_loc}/the_ville/visuals\"\n",
        "\n",
        "fs_storage = \"../../environment/frontend_server/storage\"\n",
        "fs_temp_storage = \"../../environment/frontend_server/temp_storage\"\n",
        "\n",
        "collision_block_id = \"32125\"\n",
        "\n",
        "# Verbose\n",
        "debug = True\n",
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start the frontend server"
      ],
      "metadata": {
        "id": "oWBpvEMFWyMC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTxEzbudylkq"
      },
      "outputs": [],
      "source": [
        "%cd /content/generative_agents_llama/environment/frontend_server\n",
        "\n",
        "!nohup python manage.py runserver 127.0.0.1:8000 &\n",
        "!echo \"$(<nohup.out )\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The server is running in http://localhost:8000"
      ],
      "metadata": {
        "id": "XI0r9fGDdwn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For Colab\n",
        " If you are running this notebook in Colab, you need to create an ngrok tunnel to access the frontend."
      ],
      "metadata": {
        "id": "yywN9mJ0W4ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "authtoken_ngrok = \"\" # @param {type:\"string\"}\n",
        "!ngrok config add-authtoken {authtoken_ngrok}\n",
        "\n",
        "http_tunnel = ngrok.connect(\"http://localhost:8000\")\n",
        "http_tunnel"
      ],
      "metadata": {
        "id": "OW-IkfyVDzVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open the NgrokTunnel for see the frontend"
      ],
      "metadata": {
        "id": "l1nYbNpYdplg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start the backend_server"
      ],
      "metadata": {
        "id": "o7_MtZ7WdVBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/generative_agents_llama/reverie/backend_server/"
      ],
      "metadata": {
        "id": "jPIpd8Ru_d6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the model is loaded:\n",
        "\n",
        "1. Enter name of forked simulation: `base_the_ville_isabella_maria_klaus`\n",
        "2. Enter name of new simulation: `my-simulation1`\n",
        "3. Enter the number of steps you want to run the simulation: `run 10` to run 10 steps"
      ],
      "metadata": {
        "id": "wcBvR8Gbd-EL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python reverie.py"
      ],
      "metadata": {
        "id": "Sz0mqS6HJkp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For see the simulation Go to: `http://localhost:8000/simulator_home` or your `<NgrokTunnel>/simulator_home`\n",
        "- To replay, `http://localhost:8000/replay/my-simulation1/1/` or your `<NgrokTunnel>/replay/my-simulation1/1/`"
      ],
      "metadata": {
        "id": "IiiG2nPye5dG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes\n",
        "- Issues with generating motion in the frontend still unresolved.\n",
        "- The model might not generate proper responses, which could lead to errors."
      ],
      "metadata": {
        "id": "0OxNej6mgKgY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}